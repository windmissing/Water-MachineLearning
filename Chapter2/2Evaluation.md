理想的解决方案是：对候选模型的泛化误差进行评估  
但是：无法直接获取泛化误差，而训练误差由于存在过拟合现象而不适合作为标准。  

# 2.2 评估方法

测试集 --> 测试误差 --> 泛化误差的近似  

## 2.2.1 留出法

数据集D = 训练集S + 测试集T  

1. 尽可能保持数据分布的一致性，例如：分层采样，保留类别比例  
2. 单次使用留出法得到的估计结果不够稳定可靠，多次随机划分，重复实验取平均值  
3. 若T小，评估结果不够稳定准确，若T大，则D与S差别大，评估结构不真实（保真性）。通常S=2/3D或4/5D  

## 2.2.2 交叉验证

数据集D = D1 + D2 + ... + Dk  
训练集： k-1个子集的并集  
测试集： 余下的子集  

进行k次训练和测试取平均值。  

1. 评估结果的稳定性的保真性取决于k  
2. 多次随机划分，重复实验，取平均值  
3. 当k=m时，称为留一法。优点（1）不受随机划分的影响（2）D和S相似，结果比较准确缺点（1）m大时开销巨大（2）未必永远比其他评估方法准确NFL  

2.2.3 自助法  

数据集D：m个样本  
数据集D'：从D中有放回取样m次  

$$
\lim_{m\rightarrow\infty}(1-\frac{1}{m})^m \approx 36.8\%
$$

约36.8%的样本采不到，可用于测试  

1. m小，难以有效地划分S/T时很有用  
2. 可以产生多种不同的训练集，对集成学习有很大好处  
3. D'改变了D的分布，引入地估计偏差  
4. 数据量足够时，留出法和交叉验证法更常用。  

## 2.2.4 调参与最终模型

模型选择完成后，学习算法和参数配置已选定。此时应该用D重新训练模型。  